{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brave-hamilton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim.downloader as model_api\n",
    "import statsmodels.api as sm\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-transparency",
   "metadata": {},
   "source": [
    "# 1. Sentiment analysis\n",
    "\n",
    "Using the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), we want to do a regression model that predict the ratings are on a 1-10 scale. You have an example train and test set in the `dataset` folder.\n",
    "\n",
    "### 1.1 Regression Model\n",
    "\n",
    "Use a feedforward neural network and NLP techniques we've seen up to now to train the best model you can on this dataset\n",
    "\n",
    "### 1.2 RNN model\n",
    "\n",
    "Train a RNN to do the sentiment analysis regression. The RNN should consist simply of an embedding layer (to make word IDs into word vectors) a recurrent blocks (GRU or LSTM) feeding into an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this first code after downloading dataset to get all the files\n",
    "\n",
    "# import tarfile\n",
    "# tf = tarfile.open('dataset/aclImdb_v1.tar.gz', \"r:gz\")\n",
    "# tf.extractall()\n",
    "# tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "overall-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "train_review = []\n",
    "test_review = []\n",
    "train_rating = []\n",
    "test_rating = []\n",
    "train_sent = []\n",
    "test_sent = []\n",
    "\n",
    "def list_maker(train_test, pos_neg):\n",
    "    \n",
    "    if train_test == \"train\" and pos_neg == 'pos':\n",
    "        loc = \"aclImdb/train/pos/\"\n",
    "    elif train_test == 'train' and pos_neg == 'neg':\n",
    "        loc = \"aclImdb/train/neg/\"\n",
    "    elif train_test == 'test' and pos_neg == 'pos':\n",
    "        loc = \"aclImdb/test/pos/\"\n",
    "    else:\n",
    "        loc = \"aclImdb/test/neg/\"\n",
    "    \n",
    "    for i in range(0, 12499):\n",
    "        for j in range(1, 10):\n",
    "            try:\n",
    "                f = open(loc+'%s_%s.txt' % (i, j))\n",
    "                data = f.read()\n",
    "            except:\n",
    "                continue\n",
    "            if train_test == \"train\":\n",
    "                train.append(i)\n",
    "                train_review.append(data)\n",
    "                train_rating.append(j)\n",
    "                if pos_neg == 'pos':\n",
    "                    train_sent.append(1)\n",
    "                else:\n",
    "                    train_sent.append(0)\n",
    "            else:\n",
    "                test.append(i)\n",
    "                test_review.append(data)\n",
    "                test_rating.append(j)\n",
    "                if pos_neg == 'pos':\n",
    "                    test_sent.append(1)\n",
    "                else:\n",
    "                    test_sent.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "differential-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_maker('train', 'pos')\n",
    "list_maker('train', 'neg')\n",
    "list_maker('test', 'pos')\n",
    "list_maker('test', 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "functional-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'ID':train, 'Review':train_review, 'Rating':train_rating, 'Sentiment':train_sent}).set_index('ID')\n",
    "test_df = pd.DataFrame({'ID':test, 'Review':test_review, 'Rating':test_rating, 'Sentiment':test_sent}).set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "checked-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you like adult comedy cartoons, like South ...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bromwell High is nothing short of brilliant. E...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FUTZ is the only show preserved from the exper...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Although I didn't like Stanley &amp; Iris tremendo...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12494</th>\n",
       "      <td>There was nothing about this movie that I like...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>OK, I love bad horror. I especially love horro...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>To be brutally honest... I LOVED watching Seve...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>I'm sure that the folks on the Texas/Louisiana...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12498</th>\n",
       "      <td>This film has the kernel of a really good stor...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20252 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating  Sentiment\n",
       "ID                                                                         \n",
       "0      Bromwell High is a cartoon comedy. It ran at t...       9          1\n",
       "1      If you like adult comedy cartoons, like South ...       7          1\n",
       "2      Bromwell High is nothing short of brilliant. E...       9          1\n",
       "4      FUTZ is the only show preserved from the exper...       8          1\n",
       "7      Although I didn't like Stanley & Iris tremendo...       7          1\n",
       "...                                                  ...     ...        ...\n",
       "12494  There was nothing about this movie that I like...       1          0\n",
       "12495  OK, I love bad horror. I especially love horro...       1          0\n",
       "12496  To be brutally honest... I LOVED watching Seve...       1          0\n",
       "12497  I'm sure that the folks on the Texas/Louisiana...       4          0\n",
       "12498  This film has the kernel of a really good stor...       2          0\n",
       "\n",
       "[20252 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "guilty-rally",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My yardstick for measuring a movie's watch-abi...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many movies are there that you can think o...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I only went to see this movie because I have a...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I was fortunate enough to see this movie on pr...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Our family (and the entire sold out sneak prev...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12494</th>\n",
       "      <td>This movie was obviously made with a very low ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>CyberTracker is set in Los Angeles sometime in...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>Eric Phillips (Don Wilson) is a secret service...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>Plot Synopsis: Los Angeles in the future. Crim...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12498</th>\n",
       "      <td>Oh, dear! This has to be one of the worst film...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19988 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating  Sentiment\n",
       "ID                                                                         \n",
       "2      My yardstick for measuring a movie's watch-abi...       7          1\n",
       "3      How many movies are there that you can think o...       7          1\n",
       "5      I only went to see this movie because I have a...       7          1\n",
       "6      I was fortunate enough to see this movie on pr...       7          1\n",
       "7      Our family (and the entire sold out sneak prev...       9          1\n",
       "...                                                  ...     ...        ...\n",
       "12494  This movie was obviously made with a very low ...       2          0\n",
       "12495  CyberTracker is set in Los Angeles sometime in...       3          0\n",
       "12496  Eric Phillips (Don Wilson) is a secret service...       3          0\n",
       "12497  Plot Synopsis: Los Angeles in the future. Crim...       4          0\n",
       "12498  Oh, dear! This has to be one of the worst film...       1          0\n",
       "\n",
       "[19988 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contrary-pastor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2509</th>\n",
       "      <th>2510</th>\n",
       "      <th>2511</th>\n",
       "      <th>2512</th>\n",
       "      <th>2513</th>\n",
       "      <th>2514</th>\n",
       "      <th>2515</th>\n",
       "      <th>2516</th>\n",
       "      <th>2517</th>\n",
       "      <th>2518</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell</td>\n",
       "      <td>high</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>cartoon</td>\n",
       "      <td>comedy</td>\n",
       "      <td>it</td>\n",
       "      <td>ran</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if</td>\n",
       "      <td>you</td>\n",
       "      <td>like</td>\n",
       "      <td>adult</td>\n",
       "      <td>comedy</td>\n",
       "      <td>cartoons</td>\n",
       "      <td>like</td>\n",
       "      <td>south</td>\n",
       "      <td>park</td>\n",
       "      <td>then</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bromwell</td>\n",
       "      <td>high</td>\n",
       "      <td>is</td>\n",
       "      <td>nothing</td>\n",
       "      <td>short</td>\n",
       "      <td>of</td>\n",
       "      <td>brilliant</td>\n",
       "      <td>expertly</td>\n",
       "      <td>scripted</td>\n",
       "      <td>and</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>futz</td>\n",
       "      <td>is</td>\n",
       "      <td>the</td>\n",
       "      <td>only</td>\n",
       "      <td>show</td>\n",
       "      <td>preserved</td>\n",
       "      <td>from</td>\n",
       "      <td>the</td>\n",
       "      <td>experimental</td>\n",
       "      <td>theatre</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>although</td>\n",
       "      <td>i</td>\n",
       "      <td>didn</td>\n",
       "      <td>t</td>\n",
       "      <td>like</td>\n",
       "      <td>stanley</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>iris</td>\n",
       "      <td>tremendously</td>\n",
       "      <td>as</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20247</th>\n",
       "      <td>there</td>\n",
       "      <td>was</td>\n",
       "      <td>nothing</td>\n",
       "      <td>about</td>\n",
       "      <td>this</td>\n",
       "      <td>movie</td>\n",
       "      <td>that</td>\n",
       "      <td>i</td>\n",
       "      <td>liked</td>\n",
       "      <td>it</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20248</th>\n",
       "      <td>ok</td>\n",
       "      <td>i</td>\n",
       "      <td>love</td>\n",
       "      <td>bad</td>\n",
       "      <td>horror</td>\n",
       "      <td>i</td>\n",
       "      <td>especially</td>\n",
       "      <td>love</td>\n",
       "      <td>horror</td>\n",
       "      <td>bad</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20249</th>\n",
       "      <td>to</td>\n",
       "      <td>be</td>\n",
       "      <td>brutally</td>\n",
       "      <td>honest</td>\n",
       "      <td>i</td>\n",
       "      <td>loved</td>\n",
       "      <td>watching</td>\n",
       "      <td>severed</td>\n",
       "      <td>that</td>\n",
       "      <td>s</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250</th>\n",
       "      <td>i</td>\n",
       "      <td>m</td>\n",
       "      <td>sure</td>\n",
       "      <td>that</td>\n",
       "      <td>the</td>\n",
       "      <td>folks</td>\n",
       "      <td>on</td>\n",
       "      <td>the</td>\n",
       "      <td>texas</td>\n",
       "      <td>louisiana</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20251</th>\n",
       "      <td>this</td>\n",
       "      <td>film</td>\n",
       "      <td>has</td>\n",
       "      <td>the</td>\n",
       "      <td>kernel</td>\n",
       "      <td>of</td>\n",
       "      <td>a</td>\n",
       "      <td>really</td>\n",
       "      <td>good</td>\n",
       "      <td>story</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20252 rows × 2519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1         2        3        4          5           6     \\\n",
       "0      bromwell  high        is        a  cartoon     comedy          it   \n",
       "1            if   you      like    adult   comedy   cartoons        like   \n",
       "2      bromwell  high        is  nothing    short         of   brilliant   \n",
       "3          futz    is       the     only     show  preserved        from   \n",
       "4      although     i      didn        t     like    stanley           &   \n",
       "...         ...   ...       ...      ...      ...        ...         ...   \n",
       "20247     there   was   nothing    about     this      movie        that   \n",
       "20248        ok     i      love      bad   horror          i  especially   \n",
       "20249        to    be  brutally   honest        i      loved    watching   \n",
       "20250         i     m      sure     that      the      folks          on   \n",
       "20251      this  film       has      the   kernel         of           a   \n",
       "\n",
       "           7             8          9     ...  2509  2510  2511  2512  2513  \\\n",
       "0           ran            at        the  ...  None  None  None  None  None   \n",
       "1         south          park       then  ...  None  None  None  None  None   \n",
       "2      expertly      scripted        and  ...  None  None  None  None  None   \n",
       "3           the  experimental    theatre  ...  None  None  None  None  None   \n",
       "4          iris  tremendously         as  ...  None  None  None  None  None   \n",
       "...         ...           ...        ...  ...   ...   ...   ...   ...   ...   \n",
       "20247         i         liked         it  ...  None  None  None  None  None   \n",
       "20248      love        horror        bad  ...  None  None  None  None  None   \n",
       "20249   severed          that          s  ...  None  None  None  None  None   \n",
       "20250       the         texas  louisiana  ...  None  None  None  None  None   \n",
       "20251    really          good      story  ...  None  None  None  None  None   \n",
       "\n",
       "       2514  2515  2516  2517  2518  \n",
       "0      None  None  None  None  None  \n",
       "1      None  None  None  None  None  \n",
       "2      None  None  None  None  None  \n",
       "3      None  None  None  None  None  \n",
       "4      None  None  None  None  None  \n",
       "...     ...   ...   ...   ...   ...  \n",
       "20247  None  None  None  None  None  \n",
       "20248  None  None  None  None  None  \n",
       "20249  None  None  None  None  None  \n",
       "20250  None  None  None  None  None  \n",
       "20251  None  None  None  None  None  \n",
       "\n",
       "[20252 rows x 2519 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = train_df.Review.str.split()\n",
    "words = pd.DataFrame(words.tolist())\n",
    "replaceDict = dict({\n",
    "'{':\" \", '}':\" \", ',':\"\", '.':\" \", '!':\" \", '\\\\':\" \", '/':\" \", '$':\" \", '%':\" \",\n",
    "'^':\" \", '?':\" \", '\\'':\" \", '\"':\" \", '(':\" \", ')':\" \", '*':\" \", '+':\" \", '-':\" \",\n",
    "'=':\" \", ':':\" \", ';':\" \", ']':\" \", '[':\" \", '`':\" \", '~':\" \",\n",
    "})\n",
    "\n",
    "rep = dict((re.escape(k), v) for k, v in replaceDict.items())\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "def replacer(text):\n",
    "    return rep[re.escape(text.group(0))]\n",
    "\n",
    "words = train_df.Review.str.replace(pattern, replacer).str.lower().str.split()\n",
    "words = pd.DataFrame(words.tolist())\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "premier-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-e5b5b2e3b9e6>:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word_vectors = model.wv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-22.084286</td>\n",
       "      <td>16.694746</td>\n",
       "      <td>-2.415143</td>\n",
       "      <td>-14.841063</td>\n",
       "      <td>-7.464311</td>\n",
       "      <td>4.190967</td>\n",
       "      <td>-0.289277</td>\n",
       "      <td>-5.106480</td>\n",
       "      <td>3.760873</td>\n",
       "      <td>-197.139420</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.031199</td>\n",
       "      <td>-34.052475</td>\n",
       "      <td>-3.506387</td>\n",
       "      <td>0.345735</td>\n",
       "      <td>23.491659</td>\n",
       "      <td>-5.969329</td>\n",
       "      <td>10.096868</td>\n",
       "      <td>-22.715826</td>\n",
       "      <td>-12.451815</td>\n",
       "      <td>5.562483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-16.515263</td>\n",
       "      <td>0.927775</td>\n",
       "      <td>-6.981287</td>\n",
       "      <td>-16.924288</td>\n",
       "      <td>-6.855952</td>\n",
       "      <td>9.484585</td>\n",
       "      <td>-9.155759</td>\n",
       "      <td>11.153566</td>\n",
       "      <td>13.699842</td>\n",
       "      <td>-200.510237</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.141555</td>\n",
       "      <td>-22.964143</td>\n",
       "      <td>1.434592</td>\n",
       "      <td>7.527663</td>\n",
       "      <td>14.549292</td>\n",
       "      <td>6.405801</td>\n",
       "      <td>6.753412</td>\n",
       "      <td>-16.083237</td>\n",
       "      <td>-7.338444</td>\n",
       "      <td>10.124346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20247</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20248</th>\n",
       "      <td>-17.221717</td>\n",
       "      <td>7.736229</td>\n",
       "      <td>-5.489116</td>\n",
       "      <td>-18.015010</td>\n",
       "      <td>-6.146629</td>\n",
       "      <td>7.013818</td>\n",
       "      <td>-7.566385</td>\n",
       "      <td>8.646930</td>\n",
       "      <td>5.584627</td>\n",
       "      <td>-192.853136</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.436579</td>\n",
       "      <td>-22.558603</td>\n",
       "      <td>-0.561796</td>\n",
       "      <td>5.329777</td>\n",
       "      <td>17.732513</td>\n",
       "      <td>4.175543</td>\n",
       "      <td>7.345599</td>\n",
       "      <td>-19.330603</td>\n",
       "      <td>-11.346761</td>\n",
       "      <td>11.988172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20249</th>\n",
       "      <td>-10.199886</td>\n",
       "      <td>7.659323</td>\n",
       "      <td>-4.634923</td>\n",
       "      <td>-10.346042</td>\n",
       "      <td>-10.050541</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>-8.563902</td>\n",
       "      <td>4.170983</td>\n",
       "      <td>4.856806</td>\n",
       "      <td>-142.473457</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.676579</td>\n",
       "      <td>-14.615202</td>\n",
       "      <td>-8.306254</td>\n",
       "      <td>1.860558</td>\n",
       "      <td>7.264510</td>\n",
       "      <td>-0.764165</td>\n",
       "      <td>9.424790</td>\n",
       "      <td>-7.327550</td>\n",
       "      <td>-4.098274</td>\n",
       "      <td>7.761431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250</th>\n",
       "      <td>-15.619201</td>\n",
       "      <td>20.506126</td>\n",
       "      <td>-8.568425</td>\n",
       "      <td>-17.800774</td>\n",
       "      <td>-9.581174</td>\n",
       "      <td>11.747716</td>\n",
       "      <td>-7.825003</td>\n",
       "      <td>12.141813</td>\n",
       "      <td>6.439943</td>\n",
       "      <td>-181.086682</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732178</td>\n",
       "      <td>-27.940663</td>\n",
       "      <td>-2.633032</td>\n",
       "      <td>0.330947</td>\n",
       "      <td>8.049236</td>\n",
       "      <td>29.449386</td>\n",
       "      <td>4.213533</td>\n",
       "      <td>-28.495936</td>\n",
       "      <td>-12.922284</td>\n",
       "      <td>13.916322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20251</th>\n",
       "      <td>-12.046017</td>\n",
       "      <td>18.025379</td>\n",
       "      <td>2.156377</td>\n",
       "      <td>-28.310820</td>\n",
       "      <td>-5.767010</td>\n",
       "      <td>9.778736</td>\n",
       "      <td>-16.155546</td>\n",
       "      <td>12.780479</td>\n",
       "      <td>1.738863</td>\n",
       "      <td>-181.341446</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.259562</td>\n",
       "      <td>-19.550041</td>\n",
       "      <td>-8.320841</td>\n",
       "      <td>-1.025150</td>\n",
       "      <td>6.140759</td>\n",
       "      <td>18.268064</td>\n",
       "      <td>3.891610</td>\n",
       "      <td>-13.264935</td>\n",
       "      <td>-19.602720</td>\n",
       "      <td>7.450950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20252 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2          3          4          5    \\\n",
       "0     -22.084286  16.694746 -2.415143 -14.841063  -7.464311   4.190967   \n",
       "1       0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "2       0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "3     -16.515263   0.927775 -6.981287 -16.924288  -6.855952   9.484585   \n",
       "4       0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "...          ...        ...       ...        ...        ...        ...   \n",
       "20247   0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "20248 -17.221717   7.736229 -5.489116 -18.015010  -6.146629   7.013818   \n",
       "20249 -10.199886   7.659323 -4.634923 -10.346042 -10.050541   0.025960   \n",
       "20250 -15.619201  20.506126 -8.568425 -17.800774  -9.581174  11.747716   \n",
       "20251 -12.046017  18.025379  2.156377 -28.310820  -5.767010   9.778736   \n",
       "\n",
       "             6          7          8           9    ...        290        291  \\\n",
       "0      -0.289277  -5.106480   3.760873 -197.139420  ...  -2.031199 -34.052475   \n",
       "1       0.000000   0.000000   0.000000    0.000000  ...   0.000000   0.000000   \n",
       "2       0.000000   0.000000   0.000000    0.000000  ...   0.000000   0.000000   \n",
       "3      -9.155759  11.153566  13.699842 -200.510237  ... -11.141555 -22.964143   \n",
       "4       0.000000   0.000000   0.000000    0.000000  ...   0.000000   0.000000   \n",
       "...          ...        ...        ...         ...  ...        ...        ...   \n",
       "20247   0.000000   0.000000   0.000000    0.000000  ...   0.000000   0.000000   \n",
       "20248  -7.566385   8.646930   5.584627 -192.853136  ...  -2.436579 -22.558603   \n",
       "20249  -8.563902   4.170983   4.856806 -142.473457  ...  -4.676579 -14.615202   \n",
       "20250  -7.825003  12.141813   6.439943 -181.086682  ...   2.732178 -27.940663   \n",
       "20251 -16.155546  12.780479   1.738863 -181.341446  ...  -8.259562 -19.550041   \n",
       "\n",
       "            292       293        294        295        296        297  \\\n",
       "0     -3.506387  0.345735  23.491659  -5.969329  10.096868 -22.715826   \n",
       "1      0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2      0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3      1.434592  7.527663  14.549292   6.405801   6.753412 -16.083237   \n",
       "4      0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "...         ...       ...        ...        ...        ...        ...   \n",
       "20247  0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "20248 -0.561796  5.329777  17.732513   4.175543   7.345599 -19.330603   \n",
       "20249 -8.306254  1.860558   7.264510  -0.764165   9.424790  -7.327550   \n",
       "20250 -2.633032  0.330947   8.049236  29.449386   4.213533 -28.495936   \n",
       "20251 -8.320841 -1.025150   6.140759  18.268064   3.891610 -13.264935   \n",
       "\n",
       "             298        299  \n",
       "0     -12.451815   5.562483  \n",
       "1       0.000000   0.000000  \n",
       "2       0.000000   0.000000  \n",
       "3      -7.338444  10.124346  \n",
       "4       0.000000   0.000000  \n",
       "...          ...        ...  \n",
       "20247   0.000000   0.000000  \n",
       "20248 -11.346761  11.988172  \n",
       "20249  -4.098274   7.761431  \n",
       "20250 -12.922284  13.916322  \n",
       "20251 -19.602720   7.450950  \n",
       "\n",
       "[20252 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "def soft_get(w):\n",
    "    try:\n",
    "        return word_vectors[w]\n",
    "    except KeyError:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "\n",
    "def map_vectors(row):\n",
    "    try:\n",
    "        return np.sum(\n",
    "            row.loc[words.iloc[0].notna()].apply(soft_get)\n",
    "        )\n",
    "    except:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "\n",
    "emb = pd.DataFrame(words.apply(map_vectors, axis=1).tolist())\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "competitive-return",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Rating</td>      <th>  R-squared:         </th> <td>   0.291</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.281</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   51.82</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 03 May 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:13:33</td>     <th>  Log-Likelihood:    </th> <td> -47419.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 20252</td>      <th>  AIC:               </th> <td>9.544e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 19951</td>      <th>  BIC:               </th> <td>9.782e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   300</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC1</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.2180</td> <td>    0.040</td> <td>  105.644</td> <td> 0.000</td> <td>    4.140</td> <td>    4.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>     <td>    0.0806</td> <td>    0.021</td> <td>    3.814</td> <td> 0.000</td> <td>    0.039</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>     <td>   -0.0546</td> <td>    0.023</td> <td>   -2.414</td> <td> 0.016</td> <td>   -0.099</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>     <td>   -0.0069</td> <td>    0.023</td> <td>   -0.298</td> <td> 0.766</td> <td>   -0.052</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>     <td>    0.0104</td> <td>    0.021</td> <td>    0.494</td> <td> 0.622</td> <td>   -0.031</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>     <td>   -0.0105</td> <td>    0.021</td> <td>   -0.508</td> <td> 0.611</td> <td>   -0.051</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>     <td>   -0.0814</td> <td>    0.021</td> <td>   -3.811</td> <td> 0.000</td> <td>   -0.123</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>     <td>   -0.0161</td> <td>    0.019</td> <td>   -0.828</td> <td> 0.408</td> <td>   -0.054</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>     <td>   -0.0118</td> <td>    0.021</td> <td>   -0.561</td> <td> 0.575</td> <td>   -0.053</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>     <td>   -0.0876</td> <td>    0.021</td> <td>   -4.203</td> <td> 0.000</td> <td>   -0.128</td> <td>   -0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>     <td>   -0.0016</td> <td>    0.018</td> <td>   -0.090</td> <td> 0.928</td> <td>   -0.036</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>    <td>    0.0398</td> <td>    0.023</td> <td>    1.748</td> <td> 0.080</td> <td>   -0.005</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>    <td>   -0.0133</td> <td>    0.020</td> <td>   -0.675</td> <td> 0.500</td> <td>   -0.052</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>    <td>    0.0312</td> <td>    0.021</td> <td>    1.456</td> <td> 0.145</td> <td>   -0.011</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>    <td>    0.0359</td> <td>    0.022</td> <td>    1.614</td> <td> 0.107</td> <td>   -0.008</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14</th>    <td>   -0.0478</td> <td>    0.023</td> <td>   -2.052</td> <td> 0.040</td> <td>   -0.093</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>    <td>    0.0121</td> <td>    0.019</td> <td>    0.650</td> <td> 0.516</td> <td>   -0.024</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>    <td>   -0.0074</td> <td>    0.021</td> <td>   -0.350</td> <td> 0.726</td> <td>   -0.049</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th>    <td>   -0.0486</td> <td>    0.020</td> <td>   -2.451</td> <td> 0.014</td> <td>   -0.088</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>    <td>    0.0588</td> <td>    0.025</td> <td>    2.355</td> <td> 0.019</td> <td>    0.010</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>    <td>    0.0175</td> <td>    0.021</td> <td>    0.838</td> <td> 0.402</td> <td>   -0.023</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>    <td>   -0.0443</td> <td>    0.022</td> <td>   -2.050</td> <td> 0.040</td> <td>   -0.087</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>    <td>    0.1012</td> <td>    0.021</td> <td>    4.823</td> <td> 0.000</td> <td>    0.060</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th>    <td>   -0.0144</td> <td>    0.025</td> <td>   -0.580</td> <td> 0.562</td> <td>   -0.063</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>    <td>    0.0386</td> <td>    0.023</td> <td>    1.709</td> <td> 0.087</td> <td>   -0.006</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24</th>    <td>    0.0170</td> <td>    0.020</td> <td>    0.838</td> <td> 0.402</td> <td>   -0.023</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25</th>    <td>   -0.0438</td> <td>    0.021</td> <td>   -2.054</td> <td> 0.040</td> <td>   -0.086</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>26</th>    <td>   -0.0563</td> <td>    0.023</td> <td>   -2.433</td> <td> 0.015</td> <td>   -0.102</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>27</th>    <td>    0.0327</td> <td>    0.020</td> <td>    1.609</td> <td> 0.108</td> <td>   -0.007</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28</th>    <td>   -0.0154</td> <td>    0.022</td> <td>   -0.711</td> <td> 0.477</td> <td>   -0.058</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29</th>    <td>   -0.0361</td> <td>    0.020</td> <td>   -1.776</td> <td> 0.076</td> <td>   -0.076</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>30</th>    <td>   -0.0420</td> <td>    0.017</td> <td>   -2.429</td> <td> 0.015</td> <td>   -0.076</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>31</th>    <td>   -0.0464</td> <td>    0.023</td> <td>   -1.991</td> <td> 0.046</td> <td>   -0.092</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>32</th>    <td>   -0.0159</td> <td>    0.022</td> <td>   -0.725</td> <td> 0.469</td> <td>   -0.059</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>33</th>    <td>   -0.0011</td> <td>    0.023</td> <td>   -0.050</td> <td> 0.960</td> <td>   -0.046</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>34</th>    <td>    0.0302</td> <td>    0.027</td> <td>    1.137</td> <td> 0.256</td> <td>   -0.022</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>35</th>    <td>    0.0297</td> <td>    0.021</td> <td>    1.435</td> <td> 0.151</td> <td>   -0.011</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>36</th>    <td>    0.0085</td> <td>    0.020</td> <td>    0.429</td> <td> 0.668</td> <td>   -0.030</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>37</th>    <td>   -0.0209</td> <td>    0.019</td> <td>   -1.098</td> <td> 0.272</td> <td>   -0.058</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>38</th>    <td>    0.0186</td> <td>    0.019</td> <td>    0.970</td> <td> 0.332</td> <td>   -0.019</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>39</th>    <td>   -0.0223</td> <td>    0.022</td> <td>   -1.023</td> <td> 0.306</td> <td>   -0.065</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>40</th>    <td>   -0.0697</td> <td>    0.021</td> <td>   -3.292</td> <td> 0.001</td> <td>   -0.111</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>41</th>    <td>    0.0250</td> <td>    0.019</td> <td>    1.292</td> <td> 0.196</td> <td>   -0.013</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>42</th>    <td>    0.0011</td> <td>    0.021</td> <td>    0.055</td> <td> 0.956</td> <td>   -0.040</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>43</th>    <td>   -0.0526</td> <td>    0.019</td> <td>   -2.729</td> <td> 0.006</td> <td>   -0.090</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>44</th>    <td>   -0.0705</td> <td>    0.020</td> <td>   -3.458</td> <td> 0.001</td> <td>   -0.111</td> <td>   -0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>45</th>    <td>   -0.0281</td> <td>    0.024</td> <td>   -1.179</td> <td> 0.238</td> <td>   -0.075</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>46</th>    <td>   -0.0668</td> <td>    0.025</td> <td>   -2.708</td> <td> 0.007</td> <td>   -0.115</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>47</th>    <td>    0.0031</td> <td>    0.022</td> <td>    0.139</td> <td> 0.889</td> <td>   -0.041</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>48</th>    <td>   -0.0217</td> <td>    0.022</td> <td>   -0.997</td> <td> 0.319</td> <td>   -0.064</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>49</th>    <td>    0.0467</td> <td>    0.020</td> <td>    2.288</td> <td> 0.022</td> <td>    0.007</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>50</th>    <td>    0.0438</td> <td>    0.019</td> <td>    2.339</td> <td> 0.019</td> <td>    0.007</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>51</th>    <td>    0.0023</td> <td>    0.024</td> <td>    0.097</td> <td> 0.923</td> <td>   -0.045</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>52</th>    <td>   -0.0427</td> <td>    0.019</td> <td>   -2.192</td> <td> 0.028</td> <td>   -0.081</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>53</th>    <td>    0.0201</td> <td>    0.020</td> <td>    1.006</td> <td> 0.314</td> <td>   -0.019</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>54</th>    <td>    0.0598</td> <td>    0.020</td> <td>    3.055</td> <td> 0.002</td> <td>    0.021</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>55</th>    <td>   -0.0257</td> <td>    0.023</td> <td>   -1.134</td> <td> 0.257</td> <td>   -0.070</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>56</th>    <td>    0.0115</td> <td>    0.019</td> <td>    0.598</td> <td> 0.550</td> <td>   -0.026</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>57</th>    <td>   -0.0444</td> <td>    0.021</td> <td>   -2.117</td> <td> 0.034</td> <td>   -0.086</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>58</th>    <td>    0.0383</td> <td>    0.019</td> <td>    2.007</td> <td> 0.045</td> <td>    0.001</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>59</th>    <td>   -0.0014</td> <td>    0.022</td> <td>   -0.064</td> <td> 0.949</td> <td>   -0.045</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60</th>    <td>    0.0274</td> <td>    0.020</td> <td>    1.367</td> <td> 0.172</td> <td>   -0.012</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61</th>    <td>    0.0206</td> <td>    0.024</td> <td>    0.850</td> <td> 0.395</td> <td>   -0.027</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62</th>    <td>    0.0234</td> <td>    0.023</td> <td>    1.026</td> <td> 0.305</td> <td>   -0.021</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>63</th>    <td>   -0.0145</td> <td>    0.022</td> <td>   -0.655</td> <td> 0.512</td> <td>   -0.058</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>64</th>    <td>   -0.0177</td> <td>    0.023</td> <td>   -0.776</td> <td> 0.438</td> <td>   -0.062</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>65</th>    <td> -6.54e-05</td> <td>    0.026</td> <td>   -0.003</td> <td> 0.998</td> <td>   -0.050</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>66</th>    <td>   -0.0565</td> <td>    0.021</td> <td>   -2.656</td> <td> 0.008</td> <td>   -0.098</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>67</th>    <td>    0.0554</td> <td>    0.022</td> <td>    2.524</td> <td> 0.012</td> <td>    0.012</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>68</th>    <td>   -0.0111</td> <td>    0.022</td> <td>   -0.497</td> <td> 0.619</td> <td>   -0.055</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>69</th>    <td>   -0.0260</td> <td>    0.020</td> <td>   -1.291</td> <td> 0.197</td> <td>   -0.066</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>70</th>    <td>   -0.0056</td> <td>    0.023</td> <td>   -0.245</td> <td> 0.806</td> <td>   -0.051</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>71</th>    <td>   -0.0069</td> <td>    0.020</td> <td>   -0.346</td> <td> 0.729</td> <td>   -0.046</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>72</th>    <td>    0.0226</td> <td>    0.020</td> <td>    1.138</td> <td> 0.255</td> <td>   -0.016</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>73</th>    <td>    0.0442</td> <td>    0.022</td> <td>    2.049</td> <td> 0.040</td> <td>    0.002</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>74</th>    <td>    0.0314</td> <td>    0.026</td> <td>    1.221</td> <td> 0.222</td> <td>   -0.019</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>75</th>    <td>    0.0084</td> <td>    0.023</td> <td>    0.371</td> <td> 0.711</td> <td>   -0.036</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>76</th>    <td>   -0.0172</td> <td>    0.018</td> <td>   -0.954</td> <td> 0.340</td> <td>   -0.053</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>77</th>    <td>   -0.0343</td> <td>    0.022</td> <td>   -1.579</td> <td> 0.114</td> <td>   -0.077</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>78</th>    <td>   -0.0346</td> <td>    0.020</td> <td>   -1.702</td> <td> 0.089</td> <td>   -0.075</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>79</th>    <td>   -0.0748</td> <td>    0.021</td> <td>   -3.518</td> <td> 0.000</td> <td>   -0.116</td> <td>   -0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>80</th>    <td>    0.0033</td> <td>    0.019</td> <td>    0.172</td> <td> 0.863</td> <td>   -0.034</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>81</th>    <td>   -0.0691</td> <td>    0.022</td> <td>   -3.135</td> <td> 0.002</td> <td>   -0.112</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>82</th>    <td>    0.0204</td> <td>    0.021</td> <td>    0.956</td> <td> 0.339</td> <td>   -0.021</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>83</th>    <td>   -0.0299</td> <td>    0.022</td> <td>   -1.352</td> <td> 0.176</td> <td>   -0.073</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>84</th>    <td>   -0.0353</td> <td>    0.022</td> <td>   -1.580</td> <td> 0.114</td> <td>   -0.079</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>85</th>    <td>   -0.0073</td> <td>    0.021</td> <td>   -0.348</td> <td> 0.728</td> <td>   -0.049</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>86</th>    <td>   -0.0248</td> <td>    0.022</td> <td>   -1.137</td> <td> 0.256</td> <td>   -0.067</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>87</th>    <td>   -0.0607</td> <td>    0.023</td> <td>   -2.593</td> <td> 0.010</td> <td>   -0.107</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>88</th>    <td>    0.0145</td> <td>    0.021</td> <td>    0.681</td> <td> 0.496</td> <td>   -0.027</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>89</th>    <td>   -0.0005</td> <td>    0.021</td> <td>   -0.026</td> <td> 0.980</td> <td>   -0.043</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>90</th>    <td>   -0.0006</td> <td>    0.022</td> <td>   -0.027</td> <td> 0.978</td> <td>   -0.043</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>91</th>    <td>   -0.0207</td> <td>    0.024</td> <td>   -0.847</td> <td> 0.397</td> <td>   -0.069</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>92</th>    <td>    0.0320</td> <td>    0.020</td> <td>    1.573</td> <td> 0.116</td> <td>   -0.008</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>93</th>    <td>   -0.0187</td> <td>    0.020</td> <td>   -0.923</td> <td> 0.356</td> <td>   -0.058</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>94</th>    <td>   -0.0296</td> <td>    0.022</td> <td>   -1.341</td> <td> 0.180</td> <td>   -0.073</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>95</th>    <td>   -0.0105</td> <td>    0.020</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.050</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>96</th>    <td>    0.0256</td> <td>    0.020</td> <td>    1.280</td> <td> 0.201</td> <td>   -0.014</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>97</th>    <td>   -0.0225</td> <td>    0.021</td> <td>   -1.060</td> <td> 0.289</td> <td>   -0.064</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98</th>    <td>    0.1075</td> <td>    0.022</td> <td>    4.995</td> <td> 0.000</td> <td>    0.065</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>99</th>    <td>   -0.0013</td> <td>    0.021</td> <td>   -0.061</td> <td> 0.952</td> <td>   -0.042</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>100</th>   <td>    0.0908</td> <td>    0.023</td> <td>    3.943</td> <td> 0.000</td> <td>    0.046</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>101</th>   <td>    0.0111</td> <td>    0.021</td> <td>    0.525</td> <td> 0.600</td> <td>   -0.030</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>102</th>   <td>   -0.0575</td> <td>    0.020</td> <td>   -2.905</td> <td> 0.004</td> <td>   -0.096</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>103</th>   <td>    0.0242</td> <td>    0.019</td> <td>    1.251</td> <td> 0.211</td> <td>   -0.014</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>104</th>   <td>    0.0225</td> <td>    0.020</td> <td>    1.136</td> <td> 0.256</td> <td>   -0.016</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>105</th>   <td>   -0.0037</td> <td>    0.021</td> <td>   -0.173</td> <td> 0.862</td> <td>   -0.045</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>106</th>   <td>    0.0009</td> <td>    0.022</td> <td>    0.040</td> <td> 0.968</td> <td>   -0.042</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>107</th>   <td>    0.0182</td> <td>    0.024</td> <td>    0.760</td> <td> 0.447</td> <td>   -0.029</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>108</th>   <td>    0.0133</td> <td>    0.023</td> <td>    0.583</td> <td> 0.560</td> <td>   -0.031</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>109</th>   <td>    0.0321</td> <td>    0.022</td> <td>    1.433</td> <td> 0.152</td> <td>   -0.012</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>110</th>   <td>    0.0055</td> <td>    0.021</td> <td>    0.258</td> <td> 0.796</td> <td>   -0.036</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>111</th>   <td>    0.0111</td> <td>    0.022</td> <td>    0.500</td> <td> 0.617</td> <td>   -0.032</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>112</th>   <td>    0.0446</td> <td>    0.021</td> <td>    2.162</td> <td> 0.031</td> <td>    0.004</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>113</th>   <td>    0.0049</td> <td>    0.019</td> <td>    0.262</td> <td> 0.793</td> <td>   -0.032</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114</th>   <td>    0.0306</td> <td>    0.021</td> <td>    1.430</td> <td> 0.153</td> <td>   -0.011</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>115</th>   <td>    0.0118</td> <td>    0.019</td> <td>    0.612</td> <td> 0.541</td> <td>   -0.026</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>116</th>   <td>   -0.0311</td> <td>    0.022</td> <td>   -1.440</td> <td> 0.150</td> <td>   -0.073</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>117</th>   <td>   -0.0226</td> <td>    0.022</td> <td>   -1.020</td> <td> 0.308</td> <td>   -0.066</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>118</th>   <td>   -0.0089</td> <td>    0.021</td> <td>   -0.419</td> <td> 0.675</td> <td>   -0.050</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>119</th>   <td>   -0.0500</td> <td>    0.021</td> <td>   -2.379</td> <td> 0.017</td> <td>   -0.091</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>120</th>   <td>   -0.0579</td> <td>    0.022</td> <td>   -2.597</td> <td> 0.009</td> <td>   -0.102</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>121</th>   <td>   -0.0255</td> <td>    0.021</td> <td>   -1.222</td> <td> 0.222</td> <td>   -0.066</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>122</th>   <td>    0.0768</td> <td>    0.024</td> <td>    3.187</td> <td> 0.001</td> <td>    0.030</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>123</th>   <td>    0.0010</td> <td>    0.020</td> <td>    0.049</td> <td> 0.961</td> <td>   -0.038</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>124</th>   <td>   -0.0159</td> <td>    0.021</td> <td>   -0.774</td> <td> 0.439</td> <td>   -0.056</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>125</th>   <td>   -0.0326</td> <td>    0.020</td> <td>   -1.602</td> <td> 0.109</td> <td>   -0.072</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>126</th>   <td>    0.0917</td> <td>    0.021</td> <td>    4.378</td> <td> 0.000</td> <td>    0.051</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>127</th>   <td>    0.0509</td> <td>    0.017</td> <td>    3.074</td> <td> 0.002</td> <td>    0.018</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>128</th>   <td>    0.0403</td> <td>    0.024</td> <td>    1.664</td> <td> 0.096</td> <td>   -0.007</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>129</th>   <td>   -0.0319</td> <td>    0.020</td> <td>   -1.629</td> <td> 0.103</td> <td>   -0.070</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>130</th>   <td>   -0.0286</td> <td>    0.020</td> <td>   -1.405</td> <td> 0.160</td> <td>   -0.068</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>131</th>   <td>    0.0319</td> <td>    0.021</td> <td>    1.518</td> <td> 0.129</td> <td>   -0.009</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>132</th>   <td>   -0.0066</td> <td>    0.019</td> <td>   -0.345</td> <td> 0.730</td> <td>   -0.044</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>133</th>   <td>   -0.0600</td> <td>    0.018</td> <td>   -3.297</td> <td> 0.001</td> <td>   -0.096</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>134</th>   <td>   -0.0131</td> <td>    0.019</td> <td>   -0.680</td> <td> 0.497</td> <td>   -0.051</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>135</th>   <td>   -0.0469</td> <td>    0.024</td> <td>   -1.969</td> <td> 0.049</td> <td>   -0.094</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>136</th>   <td>   -0.0064</td> <td>    0.021</td> <td>   -0.310</td> <td> 0.756</td> <td>   -0.047</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>137</th>   <td>    0.0016</td> <td>    0.018</td> <td>    0.087</td> <td> 0.931</td> <td>   -0.034</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>138</th>   <td>    0.0738</td> <td>    0.021</td> <td>    3.500</td> <td> 0.000</td> <td>    0.032</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>139</th>   <td>    0.0028</td> <td>    0.020</td> <td>    0.136</td> <td> 0.892</td> <td>   -0.037</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>140</th>   <td>    0.0501</td> <td>    0.023</td> <td>    2.218</td> <td> 0.027</td> <td>    0.006</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>141</th>   <td>    0.0514</td> <td>    0.019</td> <td>    2.695</td> <td> 0.007</td> <td>    0.014</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>142</th>   <td>   -0.0064</td> <td>    0.022</td> <td>   -0.291</td> <td> 0.771</td> <td>   -0.050</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>143</th>   <td>   -0.0164</td> <td>    0.022</td> <td>   -0.747</td> <td> 0.455</td> <td>   -0.060</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>144</th>   <td>   -0.0454</td> <td>    0.023</td> <td>   -1.940</td> <td> 0.052</td> <td>   -0.091</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>145</th>   <td>    0.0147</td> <td>    0.020</td> <td>    0.746</td> <td> 0.455</td> <td>   -0.024</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>146</th>   <td>   -0.0185</td> <td>    0.018</td> <td>   -1.043</td> <td> 0.297</td> <td>   -0.053</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>147</th>   <td>   -0.0178</td> <td>    0.021</td> <td>   -0.852</td> <td> 0.394</td> <td>   -0.059</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>148</th>   <td>   -0.0375</td> <td>    0.021</td> <td>   -1.773</td> <td> 0.076</td> <td>   -0.079</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>149</th>   <td>    0.0759</td> <td>    0.023</td> <td>    3.329</td> <td> 0.001</td> <td>    0.031</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>150</th>   <td>    0.0345</td> <td>    0.023</td> <td>    1.485</td> <td> 0.138</td> <td>   -0.011</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>151</th>   <td>   -0.0176</td> <td>    0.021</td> <td>   -0.819</td> <td> 0.413</td> <td>   -0.060</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>152</th>   <td>    0.0335</td> <td>    0.021</td> <td>    1.572</td> <td> 0.116</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>153</th>   <td>    0.0029</td> <td>    0.019</td> <td>    0.151</td> <td> 0.880</td> <td>   -0.035</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>154</th>   <td>    0.0273</td> <td>    0.023</td> <td>    1.185</td> <td> 0.236</td> <td>   -0.018</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>155</th>   <td>    0.0031</td> <td>    0.021</td> <td>    0.147</td> <td> 0.883</td> <td>   -0.038</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>156</th>   <td>   -0.0638</td> <td>    0.022</td> <td>   -2.872</td> <td> 0.004</td> <td>   -0.107</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>157</th>   <td>    0.0044</td> <td>    0.020</td> <td>    0.221</td> <td> 0.825</td> <td>   -0.034</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>158</th>   <td>   -0.0302</td> <td>    0.022</td> <td>   -1.380</td> <td> 0.168</td> <td>   -0.073</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>159</th>   <td>   -0.0189</td> <td>    0.024</td> <td>   -0.772</td> <td> 0.440</td> <td>   -0.067</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>160</th>   <td>    0.0054</td> <td>    0.019</td> <td>    0.292</td> <td> 0.770</td> <td>   -0.031</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>161</th>   <td>   -0.0396</td> <td>    0.021</td> <td>   -1.892</td> <td> 0.058</td> <td>   -0.081</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>162</th>   <td>   -0.0786</td> <td>    0.021</td> <td>   -3.673</td> <td> 0.000</td> <td>   -0.121</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>163</th>   <td>    0.0449</td> <td>    0.022</td> <td>    2.063</td> <td> 0.039</td> <td>    0.002</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>164</th>   <td>    0.0688</td> <td>    0.023</td> <td>    2.961</td> <td> 0.003</td> <td>    0.023</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>165</th>   <td>   -0.0401</td> <td>    0.024</td> <td>   -1.639</td> <td> 0.101</td> <td>   -0.088</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>166</th>   <td>    0.0432</td> <td>    0.021</td> <td>    2.058</td> <td> 0.040</td> <td>    0.002</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>167</th>   <td>   -0.0359</td> <td>    0.024</td> <td>   -1.519</td> <td> 0.129</td> <td>   -0.082</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>168</th>   <td>   -0.0175</td> <td>    0.018</td> <td>   -0.974</td> <td> 0.330</td> <td>   -0.053</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>169</th>   <td>   -0.0099</td> <td>    0.023</td> <td>   -0.425</td> <td> 0.670</td> <td>   -0.056</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>170</th>   <td>   -0.0202</td> <td>    0.020</td> <td>   -0.999</td> <td> 0.318</td> <td>   -0.060</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>171</th>   <td>    0.0196</td> <td>    0.020</td> <td>    0.987</td> <td> 0.324</td> <td>   -0.019</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>172</th>   <td>   -0.0030</td> <td>    0.023</td> <td>   -0.130</td> <td> 0.897</td> <td>   -0.048</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>173</th>   <td>   -0.0086</td> <td>    0.021</td> <td>   -0.408</td> <td> 0.683</td> <td>   -0.050</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>174</th>   <td>    0.0728</td> <td>    0.019</td> <td>    3.930</td> <td> 0.000</td> <td>    0.037</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>175</th>   <td>    0.0275</td> <td>    0.023</td> <td>    1.211</td> <td> 0.226</td> <td>   -0.017</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>176</th>   <td>    0.0283</td> <td>    0.019</td> <td>    1.471</td> <td> 0.141</td> <td>   -0.009</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>177</th>   <td>   -0.0018</td> <td>    0.019</td> <td>   -0.092</td> <td> 0.926</td> <td>   -0.040</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>178</th>   <td>    0.0306</td> <td>    0.021</td> <td>    1.447</td> <td> 0.148</td> <td>   -0.011</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>179</th>   <td>   -0.0365</td> <td>    0.020</td> <td>   -1.783</td> <td> 0.075</td> <td>   -0.077</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>180</th>   <td>    0.0124</td> <td>    0.022</td> <td>    0.554</td> <td> 0.579</td> <td>   -0.031</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>181</th>   <td>   -0.0035</td> <td>    0.023</td> <td>   -0.151</td> <td> 0.880</td> <td>   -0.048</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>182</th>   <td>    0.0386</td> <td>    0.021</td> <td>    1.873</td> <td> 0.061</td> <td>   -0.002</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>183</th>   <td>    0.0127</td> <td>    0.022</td> <td>    0.580</td> <td> 0.562</td> <td>   -0.030</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>184</th>   <td>   -0.0473</td> <td>    0.019</td> <td>   -2.459</td> <td> 0.014</td> <td>   -0.085</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>185</th>   <td>    0.0668</td> <td>    0.024</td> <td>    2.777</td> <td> 0.005</td> <td>    0.020</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>186</th>   <td>   -0.0588</td> <td>    0.020</td> <td>   -2.922</td> <td> 0.003</td> <td>   -0.098</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>187</th>   <td>   -0.0089</td> <td>    0.023</td> <td>   -0.385</td> <td> 0.700</td> <td>   -0.054</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>188</th>   <td>   -0.0105</td> <td>    0.020</td> <td>   -0.533</td> <td> 0.594</td> <td>   -0.049</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>189</th>   <td>    0.0547</td> <td>    0.016</td> <td>    3.406</td> <td> 0.001</td> <td>    0.023</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>190</th>   <td>   -0.0260</td> <td>    0.022</td> <td>   -1.169</td> <td> 0.242</td> <td>   -0.070</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>191</th>   <td>   -0.0072</td> <td>    0.023</td> <td>   -0.313</td> <td> 0.754</td> <td>   -0.052</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>192</th>   <td>   -0.0028</td> <td>    0.023</td> <td>   -0.117</td> <td> 0.907</td> <td>   -0.049</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>193</th>   <td>   -0.0194</td> <td>    0.024</td> <td>   -0.798</td> <td> 0.425</td> <td>   -0.067</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>194</th>   <td>   -0.0654</td> <td>    0.021</td> <td>   -3.130</td> <td> 0.002</td> <td>   -0.106</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>195</th>   <td>   -0.0058</td> <td>    0.021</td> <td>   -0.281</td> <td> 0.779</td> <td>   -0.046</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>196</th>   <td>    0.0482</td> <td>    0.022</td> <td>    2.226</td> <td> 0.026</td> <td>    0.006</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>197</th>   <td>   -0.0341</td> <td>    0.021</td> <td>   -1.616</td> <td> 0.106</td> <td>   -0.075</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>198</th>   <td>   -0.0035</td> <td>    0.017</td> <td>   -0.208</td> <td> 0.835</td> <td>   -0.037</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>199</th>   <td>   -0.0173</td> <td>    0.022</td> <td>   -0.773</td> <td> 0.439</td> <td>   -0.061</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>200</th>   <td>   -0.0156</td> <td>    0.020</td> <td>   -0.769</td> <td> 0.442</td> <td>   -0.055</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>201</th>   <td>    0.0009</td> <td>    0.020</td> <td>    0.047</td> <td> 0.963</td> <td>   -0.039</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>202</th>   <td>   -0.0032</td> <td>    0.020</td> <td>   -0.160</td> <td> 0.873</td> <td>   -0.043</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>203</th>   <td>   -0.0400</td> <td>    0.021</td> <td>   -1.910</td> <td> 0.056</td> <td>   -0.081</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>204</th>   <td>    0.0165</td> <td>    0.023</td> <td>    0.723</td> <td> 0.470</td> <td>   -0.028</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>205</th>   <td>   -0.0403</td> <td>    0.020</td> <td>   -2.007</td> <td> 0.045</td> <td>   -0.080</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>206</th>   <td>   -0.0956</td> <td>    0.020</td> <td>   -4.705</td> <td> 0.000</td> <td>   -0.135</td> <td>   -0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>207</th>   <td>    0.0151</td> <td>    0.020</td> <td>    0.754</td> <td> 0.451</td> <td>   -0.024</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>208</th>   <td>    0.0309</td> <td>    0.022</td> <td>    1.424</td> <td> 0.154</td> <td>   -0.012</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>209</th>   <td>   -0.0570</td> <td>    0.022</td> <td>   -2.617</td> <td> 0.009</td> <td>   -0.100</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>210</th>   <td>    0.0181</td> <td>    0.021</td> <td>    0.846</td> <td> 0.398</td> <td>   -0.024</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>211</th>   <td>    0.0503</td> <td>    0.020</td> <td>    2.507</td> <td> 0.012</td> <td>    0.011</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>212</th>   <td>   -0.0013</td> <td>    0.020</td> <td>   -0.064</td> <td> 0.949</td> <td>   -0.040</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>213</th>   <td>   -0.0011</td> <td>    0.018</td> <td>   -0.061</td> <td> 0.951</td> <td>   -0.036</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>214</th>   <td>   -0.0371</td> <td>    0.023</td> <td>   -1.604</td> <td> 0.109</td> <td>   -0.083</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>215</th>   <td>    0.0288</td> <td>    0.020</td> <td>    1.412</td> <td> 0.158</td> <td>   -0.011</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>216</th>   <td>   -0.0687</td> <td>    0.020</td> <td>   -3.430</td> <td> 0.001</td> <td>   -0.108</td> <td>   -0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>217</th>   <td>    0.0194</td> <td>    0.026</td> <td>    0.735</td> <td> 0.462</td> <td>   -0.032</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>218</th>   <td>   -0.0557</td> <td>    0.022</td> <td>   -2.530</td> <td> 0.011</td> <td>   -0.099</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>219</th>   <td>    0.0507</td> <td>    0.021</td> <td>    2.367</td> <td> 0.018</td> <td>    0.009</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>220</th>   <td>    0.0200</td> <td>    0.020</td> <td>    1.004</td> <td> 0.315</td> <td>   -0.019</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>221</th>   <td>   -0.0403</td> <td>    0.021</td> <td>   -1.881</td> <td> 0.060</td> <td>   -0.082</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>222</th>   <td>   -0.0517</td> <td>    0.022</td> <td>   -2.314</td> <td> 0.021</td> <td>   -0.095</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>223</th>   <td>    0.0137</td> <td>    0.020</td> <td>    0.688</td> <td> 0.491</td> <td>   -0.025</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>224</th>   <td>   -0.0903</td> <td>    0.022</td> <td>   -4.018</td> <td> 0.000</td> <td>   -0.134</td> <td>   -0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>225</th>   <td>   -0.0025</td> <td>    0.023</td> <td>   -0.111</td> <td> 0.912</td> <td>   -0.047</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>226</th>   <td>   -0.0283</td> <td>    0.021</td> <td>   -1.328</td> <td> 0.184</td> <td>   -0.070</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>227</th>   <td>    0.0051</td> <td>    0.021</td> <td>    0.237</td> <td> 0.813</td> <td>   -0.037</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>228</th>   <td>   -0.0391</td> <td>    0.021</td> <td>   -1.828</td> <td> 0.068</td> <td>   -0.081</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>229</th>   <td>   -0.0011</td> <td>    0.020</td> <td>   -0.057</td> <td> 0.955</td> <td>   -0.040</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>230</th>   <td>   -0.1054</td> <td>    0.022</td> <td>   -4.770</td> <td> 0.000</td> <td>   -0.149</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>231</th>   <td>    0.0047</td> <td>    0.023</td> <td>    0.206</td> <td> 0.837</td> <td>   -0.040</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>232</th>   <td>   -0.0002</td> <td>    0.024</td> <td>   -0.007</td> <td> 0.994</td> <td>   -0.047</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>233</th>   <td>    0.0146</td> <td>    0.022</td> <td>    0.663</td> <td> 0.507</td> <td>   -0.029</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>234</th>   <td>   -0.0122</td> <td>    0.020</td> <td>   -0.620</td> <td> 0.535</td> <td>   -0.051</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>235</th>   <td>    0.0573</td> <td>    0.019</td> <td>    3.016</td> <td> 0.003</td> <td>    0.020</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>236</th>   <td>   -0.0011</td> <td>    0.020</td> <td>   -0.056</td> <td> 0.956</td> <td>   -0.041</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>237</th>   <td>    0.0699</td> <td>    0.023</td> <td>    3.065</td> <td> 0.002</td> <td>    0.025</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>238</th>   <td>   -0.0302</td> <td>    0.021</td> <td>   -1.474</td> <td> 0.141</td> <td>   -0.070</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>239</th>   <td>    0.0212</td> <td>    0.021</td> <td>    1.007</td> <td> 0.314</td> <td>   -0.020</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>240</th>   <td>   -0.0230</td> <td>    0.022</td> <td>   -1.043</td> <td> 0.297</td> <td>   -0.066</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>241</th>   <td>    0.0064</td> <td>    0.022</td> <td>    0.284</td> <td> 0.777</td> <td>   -0.038</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>242</th>   <td>    0.0755</td> <td>    0.025</td> <td>    3.046</td> <td> 0.002</td> <td>    0.027</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>243</th>   <td>    0.0557</td> <td>    0.023</td> <td>    2.474</td> <td> 0.013</td> <td>    0.012</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>244</th>   <td>    0.0084</td> <td>    0.019</td> <td>    0.450</td> <td> 0.653</td> <td>   -0.028</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>245</th>   <td>   -0.0074</td> <td>    0.022</td> <td>   -0.336</td> <td> 0.737</td> <td>   -0.050</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>246</th>   <td>   -0.0036</td> <td>    0.021</td> <td>   -0.171</td> <td> 0.864</td> <td>   -0.045</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>247</th>   <td>    0.0288</td> <td>    0.020</td> <td>    1.406</td> <td> 0.160</td> <td>   -0.011</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>248</th>   <td>    0.0520</td> <td>    0.023</td> <td>    2.228</td> <td> 0.026</td> <td>    0.006</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>249</th>   <td>   -0.0222</td> <td>    0.022</td> <td>   -1.011</td> <td> 0.312</td> <td>   -0.065</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>250</th>   <td>   -0.0334</td> <td>    0.024</td> <td>   -1.411</td> <td> 0.158</td> <td>   -0.080</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>251</th>   <td>   -0.0366</td> <td>    0.019</td> <td>   -1.947</td> <td> 0.052</td> <td>   -0.073</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>252</th>   <td>    0.0022</td> <td>    0.019</td> <td>    0.116</td> <td> 0.908</td> <td>   -0.036</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>253</th>   <td>   -0.0245</td> <td>    0.021</td> <td>   -1.144</td> <td> 0.252</td> <td>   -0.067</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>254</th>   <td>    0.0030</td> <td>    0.018</td> <td>    0.168</td> <td> 0.866</td> <td>   -0.031</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>255</th>   <td>    0.0201</td> <td>    0.021</td> <td>    0.973</td> <td> 0.331</td> <td>   -0.020</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>256</th>   <td>    0.0360</td> <td>    0.022</td> <td>    1.609</td> <td> 0.108</td> <td>   -0.008</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>257</th>   <td>   -0.0058</td> <td>    0.020</td> <td>   -0.296</td> <td> 0.768</td> <td>   -0.044</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>258</th>   <td>    0.0127</td> <td>    0.021</td> <td>    0.591</td> <td> 0.554</td> <td>   -0.029</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>259</th>   <td>    0.0230</td> <td>    0.022</td> <td>    1.061</td> <td> 0.289</td> <td>   -0.020</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>260</th>   <td>   -0.0247</td> <td>    0.024</td> <td>   -1.044</td> <td> 0.297</td> <td>   -0.071</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>261</th>   <td>    0.0375</td> <td>    0.020</td> <td>    1.911</td> <td> 0.056</td> <td>   -0.001</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>262</th>   <td>    0.0070</td> <td>    0.021</td> <td>    0.326</td> <td> 0.745</td> <td>   -0.035</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>263</th>   <td>   -0.0319</td> <td>    0.023</td> <td>   -1.380</td> <td> 0.168</td> <td>   -0.077</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>264</th>   <td>   -0.0391</td> <td>    0.020</td> <td>   -1.910</td> <td> 0.056</td> <td>   -0.079</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>265</th>   <td>   -0.0003</td> <td>    0.022</td> <td>   -0.013</td> <td> 0.990</td> <td>   -0.043</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>266</th>   <td>   -0.0777</td> <td>    0.024</td> <td>   -3.283</td> <td> 0.001</td> <td>   -0.124</td> <td>   -0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>267</th>   <td>   -0.0063</td> <td>    0.021</td> <td>   -0.303</td> <td> 0.762</td> <td>   -0.047</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>268</th>   <td>   -0.0205</td> <td>    0.022</td> <td>   -0.952</td> <td> 0.341</td> <td>   -0.063</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>269</th>   <td>    0.0057</td> <td>    0.018</td> <td>    0.321</td> <td> 0.749</td> <td>   -0.029</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>270</th>   <td>   -0.0231</td> <td>    0.021</td> <td>   -1.121</td> <td> 0.262</td> <td>   -0.064</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>271</th>   <td>   -0.0406</td> <td>    0.019</td> <td>   -2.196</td> <td> 0.028</td> <td>   -0.077</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>272</th>   <td>   -0.0305</td> <td>    0.025</td> <td>   -1.217</td> <td> 0.224</td> <td>   -0.080</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>273</th>   <td>    0.0175</td> <td>    0.022</td> <td>    0.807</td> <td> 0.420</td> <td>   -0.025</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>274</th>   <td>   -0.0116</td> <td>    0.021</td> <td>   -0.548</td> <td> 0.584</td> <td>   -0.053</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>275</th>   <td>   -0.0069</td> <td>    0.021</td> <td>   -0.330</td> <td> 0.741</td> <td>   -0.048</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>276</th>   <td>   -0.0123</td> <td>    0.015</td> <td>   -0.820</td> <td> 0.412</td> <td>   -0.042</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>277</th>   <td>    0.0516</td> <td>    0.018</td> <td>    2.890</td> <td> 0.004</td> <td>    0.017</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>278</th>   <td>   -0.0751</td> <td>    0.025</td> <td>   -3.047</td> <td> 0.002</td> <td>   -0.123</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>279</th>   <td>   -0.0223</td> <td>    0.021</td> <td>   -1.077</td> <td> 0.281</td> <td>   -0.063</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>280</th>   <td>    0.0163</td> <td>    0.020</td> <td>    0.825</td> <td> 0.410</td> <td>   -0.022</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>281</th>   <td>    0.0301</td> <td>    0.022</td> <td>    1.349</td> <td> 0.177</td> <td>   -0.014</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>282</th>   <td>    0.0135</td> <td>    0.021</td> <td>    0.657</td> <td> 0.511</td> <td>   -0.027</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>283</th>   <td>    0.0155</td> <td>    0.021</td> <td>    0.739</td> <td> 0.460</td> <td>   -0.026</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>284</th>   <td>    0.0187</td> <td>    0.021</td> <td>    0.897</td> <td> 0.370</td> <td>   -0.022</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>285</th>   <td>   -0.0033</td> <td>    0.021</td> <td>   -0.157</td> <td> 0.876</td> <td>   -0.045</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>286</th>   <td>   -0.0399</td> <td>    0.023</td> <td>   -1.739</td> <td> 0.082</td> <td>   -0.085</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>287</th>   <td>    0.0109</td> <td>    0.023</td> <td>    0.474</td> <td> 0.636</td> <td>   -0.034</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>288</th>   <td>    0.0112</td> <td>    0.025</td> <td>    0.450</td> <td> 0.653</td> <td>   -0.038</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>289</th>   <td>   -0.0469</td> <td>    0.026</td> <td>   -1.835</td> <td> 0.066</td> <td>   -0.097</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>290</th>   <td>   -0.0403</td> <td>    0.020</td> <td>   -1.990</td> <td> 0.047</td> <td>   -0.080</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>291</th>   <td>    0.0662</td> <td>    0.022</td> <td>    2.958</td> <td> 0.003</td> <td>    0.022</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>292</th>   <td>   -0.0319</td> <td>    0.020</td> <td>   -1.624</td> <td> 0.104</td> <td>   -0.070</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>293</th>   <td>   -0.0305</td> <td>    0.018</td> <td>   -1.712</td> <td> 0.087</td> <td>   -0.065</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>294</th>   <td>   -0.0787</td> <td>    0.020</td> <td>   -3.970</td> <td> 0.000</td> <td>   -0.118</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>295</th>   <td>   -0.0292</td> <td>    0.019</td> <td>   -1.508</td> <td> 0.132</td> <td>   -0.067</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>296</th>   <td>   -0.0155</td> <td>    0.019</td> <td>   -0.838</td> <td> 0.402</td> <td>   -0.052</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>297</th>   <td>    0.0381</td> <td>    0.021</td> <td>    1.787</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>298</th>   <td>   -0.0147</td> <td>    0.023</td> <td>   -0.645</td> <td> 0.519</td> <td>   -0.059</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>299</th>   <td>   -0.0670</td> <td>    0.022</td> <td>   -3.045</td> <td> 0.002</td> <td>   -0.110</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2591.735</td> <th>  Durbin-Watson:     </th> <td>   0.699</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 910.560</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.289</td>  <th>  Prob(JB):          </th> <td>1.88e-198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 2.137</td>  <th>  Cond. No.          </th> <td>1.41e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC1)<br/>[2] The condition number is large, 1.41e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Rating   R-squared:                       0.291\n",
       "Model:                            OLS   Adj. R-squared:                  0.281\n",
       "Method:                 Least Squares   F-statistic:                     51.82\n",
       "Date:                Mon, 03 May 2021   Prob (F-statistic):               0.00\n",
       "Time:                        19:13:33   Log-Likelihood:                -47419.\n",
       "No. Observations:               20252   AIC:                         9.544e+04\n",
       "Df Residuals:                   19951   BIC:                         9.782e+04\n",
       "Df Model:                         300                                         \n",
       "Covariance Type:                  HC1                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.2180      0.040    105.644      0.000       4.140       4.296\n",
       "0              0.0806      0.021      3.814      0.000       0.039       0.122\n",
       "1             -0.0546      0.023     -2.414      0.016      -0.099      -0.010\n",
       "2             -0.0069      0.023     -0.298      0.766      -0.052       0.038\n",
       "3              0.0104      0.021      0.494      0.622      -0.031       0.051\n",
       "4             -0.0105      0.021     -0.508      0.611      -0.051       0.030\n",
       "5             -0.0814      0.021     -3.811      0.000      -0.123      -0.040\n",
       "6             -0.0161      0.019     -0.828      0.408      -0.054       0.022\n",
       "7             -0.0118      0.021     -0.561      0.575      -0.053       0.029\n",
       "8             -0.0876      0.021     -4.203      0.000      -0.128      -0.047\n",
       "9             -0.0016      0.018     -0.090      0.928      -0.036       0.033\n",
       "10             0.0398      0.023      1.748      0.080      -0.005       0.084\n",
       "11            -0.0133      0.020     -0.675      0.500      -0.052       0.025\n",
       "12             0.0312      0.021      1.456      0.145      -0.011       0.073\n",
       "13             0.0359      0.022      1.614      0.107      -0.008       0.080\n",
       "14            -0.0478      0.023     -2.052      0.040      -0.093      -0.002\n",
       "15             0.0121      0.019      0.650      0.516      -0.024       0.048\n",
       "16            -0.0074      0.021     -0.350      0.726      -0.049       0.034\n",
       "17            -0.0486      0.020     -2.451      0.014      -0.088      -0.010\n",
       "18             0.0588      0.025      2.355      0.019       0.010       0.108\n",
       "19             0.0175      0.021      0.838      0.402      -0.023       0.059\n",
       "20            -0.0443      0.022     -2.050      0.040      -0.087      -0.002\n",
       "21             0.1012      0.021      4.823      0.000       0.060       0.142\n",
       "22            -0.0144      0.025     -0.580      0.562      -0.063       0.034\n",
       "23             0.0386      0.023      1.709      0.087      -0.006       0.083\n",
       "24             0.0170      0.020      0.838      0.402      -0.023       0.057\n",
       "25            -0.0438      0.021     -2.054      0.040      -0.086      -0.002\n",
       "26            -0.0563      0.023     -2.433      0.015      -0.102      -0.011\n",
       "27             0.0327      0.020      1.609      0.108      -0.007       0.073\n",
       "28            -0.0154      0.022     -0.711      0.477      -0.058       0.027\n",
       "29            -0.0361      0.020     -1.776      0.076      -0.076       0.004\n",
       "30            -0.0420      0.017     -2.429      0.015      -0.076      -0.008\n",
       "31            -0.0464      0.023     -1.991      0.046      -0.092      -0.001\n",
       "32            -0.0159      0.022     -0.725      0.469      -0.059       0.027\n",
       "33            -0.0011      0.023     -0.050      0.960      -0.046       0.044\n",
       "34             0.0302      0.027      1.137      0.256      -0.022       0.082\n",
       "35             0.0297      0.021      1.435      0.151      -0.011       0.070\n",
       "36             0.0085      0.020      0.429      0.668      -0.030       0.047\n",
       "37            -0.0209      0.019     -1.098      0.272      -0.058       0.016\n",
       "38             0.0186      0.019      0.970      0.332      -0.019       0.056\n",
       "39            -0.0223      0.022     -1.023      0.306      -0.065       0.020\n",
       "40            -0.0697      0.021     -3.292      0.001      -0.111      -0.028\n",
       "41             0.0250      0.019      1.292      0.196      -0.013       0.063\n",
       "42             0.0011      0.021      0.055      0.956      -0.040       0.042\n",
       "43            -0.0526      0.019     -2.729      0.006      -0.090      -0.015\n",
       "44            -0.0705      0.020     -3.458      0.001      -0.111      -0.031\n",
       "45            -0.0281      0.024     -1.179      0.238      -0.075       0.019\n",
       "46            -0.0668      0.025     -2.708      0.007      -0.115      -0.018\n",
       "47             0.0031      0.022      0.139      0.889      -0.041       0.047\n",
       "48            -0.0217      0.022     -0.997      0.319      -0.064       0.021\n",
       "49             0.0467      0.020      2.288      0.022       0.007       0.087\n",
       "50             0.0438      0.019      2.339      0.019       0.007       0.081\n",
       "51             0.0023      0.024      0.097      0.923      -0.045       0.050\n",
       "52            -0.0427      0.019     -2.192      0.028      -0.081      -0.005\n",
       "53             0.0201      0.020      1.006      0.314      -0.019       0.059\n",
       "54             0.0598      0.020      3.055      0.002       0.021       0.098\n",
       "55            -0.0257      0.023     -1.134      0.257      -0.070       0.019\n",
       "56             0.0115      0.019      0.598      0.550      -0.026       0.049\n",
       "57            -0.0444      0.021     -2.117      0.034      -0.086      -0.003\n",
       "58             0.0383      0.019      2.007      0.045       0.001       0.076\n",
       "59            -0.0014      0.022     -0.064      0.949      -0.045       0.042\n",
       "60             0.0274      0.020      1.367      0.172      -0.012       0.067\n",
       "61             0.0206      0.024      0.850      0.395      -0.027       0.068\n",
       "62             0.0234      0.023      1.026      0.305      -0.021       0.068\n",
       "63            -0.0145      0.022     -0.655      0.512      -0.058       0.029\n",
       "64            -0.0177      0.023     -0.776      0.438      -0.062       0.027\n",
       "65          -6.54e-05      0.026     -0.003      0.998      -0.050       0.050\n",
       "66            -0.0565      0.021     -2.656      0.008      -0.098      -0.015\n",
       "67             0.0554      0.022      2.524      0.012       0.012       0.099\n",
       "68            -0.0111      0.022     -0.497      0.619      -0.055       0.033\n",
       "69            -0.0260      0.020     -1.291      0.197      -0.066       0.013\n",
       "70            -0.0056      0.023     -0.245      0.806      -0.051       0.039\n",
       "71            -0.0069      0.020     -0.346      0.729      -0.046       0.032\n",
       "72             0.0226      0.020      1.138      0.255      -0.016       0.061\n",
       "73             0.0442      0.022      2.049      0.040       0.002       0.086\n",
       "74             0.0314      0.026      1.221      0.222      -0.019       0.082\n",
       "75             0.0084      0.023      0.371      0.711      -0.036       0.053\n",
       "76            -0.0172      0.018     -0.954      0.340      -0.053       0.018\n",
       "77            -0.0343      0.022     -1.579      0.114      -0.077       0.008\n",
       "78            -0.0346      0.020     -1.702      0.089      -0.075       0.005\n",
       "79            -0.0748      0.021     -3.518      0.000      -0.116      -0.033\n",
       "80             0.0033      0.019      0.172      0.863      -0.034       0.040\n",
       "81            -0.0691      0.022     -3.135      0.002      -0.112      -0.026\n",
       "82             0.0204      0.021      0.956      0.339      -0.021       0.062\n",
       "83            -0.0299      0.022     -1.352      0.176      -0.073       0.013\n",
       "84            -0.0353      0.022     -1.580      0.114      -0.079       0.008\n",
       "85            -0.0073      0.021     -0.348      0.728      -0.049       0.034\n",
       "86            -0.0248      0.022     -1.137      0.256      -0.067       0.018\n",
       "87            -0.0607      0.023     -2.593      0.010      -0.107      -0.015\n",
       "88             0.0145      0.021      0.681      0.496      -0.027       0.056\n",
       "89            -0.0005      0.021     -0.026      0.980      -0.043       0.042\n",
       "90            -0.0006      0.022     -0.027      0.978      -0.043       0.042\n",
       "91            -0.0207      0.024     -0.847      0.397      -0.069       0.027\n",
       "92             0.0320      0.020      1.573      0.116      -0.008       0.072\n",
       "93            -0.0187      0.020     -0.923      0.356      -0.058       0.021\n",
       "94            -0.0296      0.022     -1.341      0.180      -0.073       0.014\n",
       "95            -0.0105      0.020     -0.523      0.601      -0.050       0.029\n",
       "96             0.0256      0.020      1.280      0.201      -0.014       0.065\n",
       "97            -0.0225      0.021     -1.060      0.289      -0.064       0.019\n",
       "98             0.1075      0.022      4.995      0.000       0.065       0.150\n",
       "99            -0.0013      0.021     -0.061      0.952      -0.042       0.040\n",
       "100            0.0908      0.023      3.943      0.000       0.046       0.136\n",
       "101            0.0111      0.021      0.525      0.600      -0.030       0.053\n",
       "102           -0.0575      0.020     -2.905      0.004      -0.096      -0.019\n",
       "103            0.0242      0.019      1.251      0.211      -0.014       0.062\n",
       "104            0.0225      0.020      1.136      0.256      -0.016       0.061\n",
       "105           -0.0037      0.021     -0.173      0.862      -0.045       0.038\n",
       "106            0.0009      0.022      0.040      0.968      -0.042       0.043\n",
       "107            0.0182      0.024      0.760      0.447      -0.029       0.065\n",
       "108            0.0133      0.023      0.583      0.560      -0.031       0.058\n",
       "109            0.0321      0.022      1.433      0.152      -0.012       0.076\n",
       "110            0.0055      0.021      0.258      0.796      -0.036       0.047\n",
       "111            0.0111      0.022      0.500      0.617      -0.032       0.055\n",
       "112            0.0446      0.021      2.162      0.031       0.004       0.085\n",
       "113            0.0049      0.019      0.262      0.793      -0.032       0.041\n",
       "114            0.0306      0.021      1.430      0.153      -0.011       0.072\n",
       "115            0.0118      0.019      0.612      0.541      -0.026       0.050\n",
       "116           -0.0311      0.022     -1.440      0.150      -0.073       0.011\n",
       "117           -0.0226      0.022     -1.020      0.308      -0.066       0.021\n",
       "118           -0.0089      0.021     -0.419      0.675      -0.050       0.033\n",
       "119           -0.0500      0.021     -2.379      0.017      -0.091      -0.009\n",
       "120           -0.0579      0.022     -2.597      0.009      -0.102      -0.014\n",
       "121           -0.0255      0.021     -1.222      0.222      -0.066       0.015\n",
       "122            0.0768      0.024      3.187      0.001       0.030       0.124\n",
       "123            0.0010      0.020      0.049      0.961      -0.038       0.040\n",
       "124           -0.0159      0.021     -0.774      0.439      -0.056       0.024\n",
       "125           -0.0326      0.020     -1.602      0.109      -0.072       0.007\n",
       "126            0.0917      0.021      4.378      0.000       0.051       0.133\n",
       "127            0.0509      0.017      3.074      0.002       0.018       0.083\n",
       "128            0.0403      0.024      1.664      0.096      -0.007       0.088\n",
       "129           -0.0319      0.020     -1.629      0.103      -0.070       0.006\n",
       "130           -0.0286      0.020     -1.405      0.160      -0.068       0.011\n",
       "131            0.0319      0.021      1.518      0.129      -0.009       0.073\n",
       "132           -0.0066      0.019     -0.345      0.730      -0.044       0.031\n",
       "133           -0.0600      0.018     -3.297      0.001      -0.096      -0.024\n",
       "134           -0.0131      0.019     -0.680      0.497      -0.051       0.025\n",
       "135           -0.0469      0.024     -1.969      0.049      -0.094      -0.000\n",
       "136           -0.0064      0.021     -0.310      0.756      -0.047       0.034\n",
       "137            0.0016      0.018      0.087      0.931      -0.034       0.038\n",
       "138            0.0738      0.021      3.500      0.000       0.032       0.115\n",
       "139            0.0028      0.020      0.136      0.892      -0.037       0.043\n",
       "140            0.0501      0.023      2.218      0.027       0.006       0.094\n",
       "141            0.0514      0.019      2.695      0.007       0.014       0.089\n",
       "142           -0.0064      0.022     -0.291      0.771      -0.050       0.037\n",
       "143           -0.0164      0.022     -0.747      0.455      -0.060       0.027\n",
       "144           -0.0454      0.023     -1.940      0.052      -0.091       0.000\n",
       "145            0.0147      0.020      0.746      0.455      -0.024       0.053\n",
       "146           -0.0185      0.018     -1.043      0.297      -0.053       0.016\n",
       "147           -0.0178      0.021     -0.852      0.394      -0.059       0.023\n",
       "148           -0.0375      0.021     -1.773      0.076      -0.079       0.004\n",
       "149            0.0759      0.023      3.329      0.001       0.031       0.121\n",
       "150            0.0345      0.023      1.485      0.138      -0.011       0.080\n",
       "151           -0.0176      0.021     -0.819      0.413      -0.060       0.025\n",
       "152            0.0335      0.021      1.572      0.116      -0.008       0.075\n",
       "153            0.0029      0.019      0.151      0.880      -0.035       0.041\n",
       "154            0.0273      0.023      1.185      0.236      -0.018       0.072\n",
       "155            0.0031      0.021      0.147      0.883      -0.038       0.044\n",
       "156           -0.0638      0.022     -2.872      0.004      -0.107      -0.020\n",
       "157            0.0044      0.020      0.221      0.825      -0.034       0.043\n",
       "158           -0.0302      0.022     -1.380      0.168      -0.073       0.013\n",
       "159           -0.0189      0.024     -0.772      0.440      -0.067       0.029\n",
       "160            0.0054      0.019      0.292      0.770      -0.031       0.042\n",
       "161           -0.0396      0.021     -1.892      0.058      -0.081       0.001\n",
       "162           -0.0786      0.021     -3.673      0.000      -0.121      -0.037\n",
       "163            0.0449      0.022      2.063      0.039       0.002       0.088\n",
       "164            0.0688      0.023      2.961      0.003       0.023       0.114\n",
       "165           -0.0401      0.024     -1.639      0.101      -0.088       0.008\n",
       "166            0.0432      0.021      2.058      0.040       0.002       0.084\n",
       "167           -0.0359      0.024     -1.519      0.129      -0.082       0.010\n",
       "168           -0.0175      0.018     -0.974      0.330      -0.053       0.018\n",
       "169           -0.0099      0.023     -0.425      0.670      -0.056       0.036\n",
       "170           -0.0202      0.020     -0.999      0.318      -0.060       0.019\n",
       "171            0.0196      0.020      0.987      0.324      -0.019       0.059\n",
       "172           -0.0030      0.023     -0.130      0.897      -0.048       0.042\n",
       "173           -0.0086      0.021     -0.408      0.683      -0.050       0.033\n",
       "174            0.0728      0.019      3.930      0.000       0.037       0.109\n",
       "175            0.0275      0.023      1.211      0.226      -0.017       0.072\n",
       "176            0.0283      0.019      1.471      0.141      -0.009       0.066\n",
       "177           -0.0018      0.019     -0.092      0.926      -0.040       0.036\n",
       "178            0.0306      0.021      1.447      0.148      -0.011       0.072\n",
       "179           -0.0365      0.020     -1.783      0.075      -0.077       0.004\n",
       "180            0.0124      0.022      0.554      0.579      -0.031       0.056\n",
       "181           -0.0035      0.023     -0.151      0.880      -0.048       0.041\n",
       "182            0.0386      0.021      1.873      0.061      -0.002       0.079\n",
       "183            0.0127      0.022      0.580      0.562      -0.030       0.055\n",
       "184           -0.0473      0.019     -2.459      0.014      -0.085      -0.010\n",
       "185            0.0668      0.024      2.777      0.005       0.020       0.114\n",
       "186           -0.0588      0.020     -2.922      0.003      -0.098      -0.019\n",
       "187           -0.0089      0.023     -0.385      0.700      -0.054       0.036\n",
       "188           -0.0105      0.020     -0.533      0.594      -0.049       0.028\n",
       "189            0.0547      0.016      3.406      0.001       0.023       0.086\n",
       "190           -0.0260      0.022     -1.169      0.242      -0.070       0.018\n",
       "191           -0.0072      0.023     -0.313      0.754      -0.052       0.038\n",
       "192           -0.0028      0.023     -0.117      0.907      -0.049       0.043\n",
       "193           -0.0194      0.024     -0.798      0.425      -0.067       0.028\n",
       "194           -0.0654      0.021     -3.130      0.002      -0.106      -0.024\n",
       "195           -0.0058      0.021     -0.281      0.779      -0.046       0.035\n",
       "196            0.0482      0.022      2.226      0.026       0.006       0.091\n",
       "197           -0.0341      0.021     -1.616      0.106      -0.075       0.007\n",
       "198           -0.0035      0.017     -0.208      0.835      -0.037       0.030\n",
       "199           -0.0173      0.022     -0.773      0.439      -0.061       0.026\n",
       "200           -0.0156      0.020     -0.769      0.442      -0.055       0.024\n",
       "201            0.0009      0.020      0.047      0.963      -0.039       0.040\n",
       "202           -0.0032      0.020     -0.160      0.873      -0.043       0.037\n",
       "203           -0.0400      0.021     -1.910      0.056      -0.081       0.001\n",
       "204            0.0165      0.023      0.723      0.470      -0.028       0.061\n",
       "205           -0.0403      0.020     -2.007      0.045      -0.080      -0.001\n",
       "206           -0.0956      0.020     -4.705      0.000      -0.135      -0.056\n",
       "207            0.0151      0.020      0.754      0.451      -0.024       0.054\n",
       "208            0.0309      0.022      1.424      0.154      -0.012       0.073\n",
       "209           -0.0570      0.022     -2.617      0.009      -0.100      -0.014\n",
       "210            0.0181      0.021      0.846      0.398      -0.024       0.060\n",
       "211            0.0503      0.020      2.507      0.012       0.011       0.090\n",
       "212           -0.0013      0.020     -0.064      0.949      -0.040       0.038\n",
       "213           -0.0011      0.018     -0.061      0.951      -0.036       0.034\n",
       "214           -0.0371      0.023     -1.604      0.109      -0.083       0.008\n",
       "215            0.0288      0.020      1.412      0.158      -0.011       0.069\n",
       "216           -0.0687      0.020     -3.430      0.001      -0.108      -0.029\n",
       "217            0.0194      0.026      0.735      0.462      -0.032       0.071\n",
       "218           -0.0557      0.022     -2.530      0.011      -0.099      -0.013\n",
       "219            0.0507      0.021      2.367      0.018       0.009       0.093\n",
       "220            0.0200      0.020      1.004      0.315      -0.019       0.059\n",
       "221           -0.0403      0.021     -1.881      0.060      -0.082       0.002\n",
       "222           -0.0517      0.022     -2.314      0.021      -0.095      -0.008\n",
       "223            0.0137      0.020      0.688      0.491      -0.025       0.053\n",
       "224           -0.0903      0.022     -4.018      0.000      -0.134      -0.046\n",
       "225           -0.0025      0.023     -0.111      0.912      -0.047       0.042\n",
       "226           -0.0283      0.021     -1.328      0.184      -0.070       0.013\n",
       "227            0.0051      0.021      0.237      0.813      -0.037       0.047\n",
       "228           -0.0391      0.021     -1.828      0.068      -0.081       0.003\n",
       "229           -0.0011      0.020     -0.057      0.955      -0.040       0.037\n",
       "230           -0.1054      0.022     -4.770      0.000      -0.149      -0.062\n",
       "231            0.0047      0.023      0.206      0.837      -0.040       0.049\n",
       "232           -0.0002      0.024     -0.007      0.994      -0.047       0.047\n",
       "233            0.0146      0.022      0.663      0.507      -0.029       0.058\n",
       "234           -0.0122      0.020     -0.620      0.535      -0.051       0.026\n",
       "235            0.0573      0.019      3.016      0.003       0.020       0.095\n",
       "236           -0.0011      0.020     -0.056      0.956      -0.041       0.039\n",
       "237            0.0699      0.023      3.065      0.002       0.025       0.115\n",
       "238           -0.0302      0.021     -1.474      0.141      -0.070       0.010\n",
       "239            0.0212      0.021      1.007      0.314      -0.020       0.062\n",
       "240           -0.0230      0.022     -1.043      0.297      -0.066       0.020\n",
       "241            0.0064      0.022      0.284      0.777      -0.038       0.050\n",
       "242            0.0755      0.025      3.046      0.002       0.027       0.124\n",
       "243            0.0557      0.023      2.474      0.013       0.012       0.100\n",
       "244            0.0084      0.019      0.450      0.653      -0.028       0.045\n",
       "245           -0.0074      0.022     -0.336      0.737      -0.050       0.036\n",
       "246           -0.0036      0.021     -0.171      0.864      -0.045       0.038\n",
       "247            0.0288      0.020      1.406      0.160      -0.011       0.069\n",
       "248            0.0520      0.023      2.228      0.026       0.006       0.098\n",
       "249           -0.0222      0.022     -1.011      0.312      -0.065       0.021\n",
       "250           -0.0334      0.024     -1.411      0.158      -0.080       0.013\n",
       "251           -0.0366      0.019     -1.947      0.052      -0.073       0.000\n",
       "252            0.0022      0.019      0.116      0.908      -0.036       0.040\n",
       "253           -0.0245      0.021     -1.144      0.252      -0.067       0.017\n",
       "254            0.0030      0.018      0.168      0.866      -0.031       0.037\n",
       "255            0.0201      0.021      0.973      0.331      -0.020       0.061\n",
       "256            0.0360      0.022      1.609      0.108      -0.008       0.080\n",
       "257           -0.0058      0.020     -0.296      0.768      -0.044       0.033\n",
       "258            0.0127      0.021      0.591      0.554      -0.029       0.055\n",
       "259            0.0230      0.022      1.061      0.289      -0.020       0.066\n",
       "260           -0.0247      0.024     -1.044      0.297      -0.071       0.022\n",
       "261            0.0375      0.020      1.911      0.056      -0.001       0.076\n",
       "262            0.0070      0.021      0.326      0.745      -0.035       0.049\n",
       "263           -0.0319      0.023     -1.380      0.168      -0.077       0.013\n",
       "264           -0.0391      0.020     -1.910      0.056      -0.079       0.001\n",
       "265           -0.0003      0.022     -0.013      0.990      -0.043       0.043\n",
       "266           -0.0777      0.024     -3.283      0.001      -0.124      -0.031\n",
       "267           -0.0063      0.021     -0.303      0.762      -0.047       0.035\n",
       "268           -0.0205      0.022     -0.952      0.341      -0.063       0.022\n",
       "269            0.0057      0.018      0.321      0.749      -0.029       0.041\n",
       "270           -0.0231      0.021     -1.121      0.262      -0.064       0.017\n",
       "271           -0.0406      0.019     -2.196      0.028      -0.077      -0.004\n",
       "272           -0.0305      0.025     -1.217      0.224      -0.080       0.019\n",
       "273            0.0175      0.022      0.807      0.420      -0.025       0.060\n",
       "274           -0.0116      0.021     -0.548      0.584      -0.053       0.030\n",
       "275           -0.0069      0.021     -0.330      0.741      -0.048       0.034\n",
       "276           -0.0123      0.015     -0.820      0.412      -0.042       0.017\n",
       "277            0.0516      0.018      2.890      0.004       0.017       0.087\n",
       "278           -0.0751      0.025     -3.047      0.002      -0.123      -0.027\n",
       "279           -0.0223      0.021     -1.077      0.281      -0.063       0.018\n",
       "280            0.0163      0.020      0.825      0.410      -0.022       0.055\n",
       "281            0.0301      0.022      1.349      0.177      -0.014       0.074\n",
       "282            0.0135      0.021      0.657      0.511      -0.027       0.054\n",
       "283            0.0155      0.021      0.739      0.460      -0.026       0.057\n",
       "284            0.0187      0.021      0.897      0.370      -0.022       0.060\n",
       "285           -0.0033      0.021     -0.157      0.876      -0.045       0.039\n",
       "286           -0.0399      0.023     -1.739      0.082      -0.085       0.005\n",
       "287            0.0109      0.023      0.474      0.636      -0.034       0.056\n",
       "288            0.0112      0.025      0.450      0.653      -0.038       0.060\n",
       "289           -0.0469      0.026     -1.835      0.066      -0.097       0.003\n",
       "290           -0.0403      0.020     -1.990      0.047      -0.080      -0.001\n",
       "291            0.0662      0.022      2.958      0.003       0.022       0.110\n",
       "292           -0.0319      0.020     -1.624      0.104      -0.070       0.007\n",
       "293           -0.0305      0.018     -1.712      0.087      -0.065       0.004\n",
       "294           -0.0787      0.020     -3.970      0.000      -0.118      -0.040\n",
       "295           -0.0292      0.019     -1.508      0.132      -0.067       0.009\n",
       "296           -0.0155      0.019     -0.838      0.402      -0.052       0.021\n",
       "297            0.0381      0.021      1.787      0.074      -0.004       0.080\n",
       "298           -0.0147      0.023     -0.645      0.519      -0.059       0.030\n",
       "299           -0.0670      0.022     -3.045      0.002      -0.110      -0.024\n",
       "==============================================================================\n",
       "Omnibus:                     2591.735   Durbin-Watson:                   0.699\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              910.560\n",
       "Skew:                           0.289   Prob(JB):                    1.88e-198\n",
       "Kurtosis:                       2.137   Cond. No.                     1.41e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
       "[2] The condition number is large, 1.41e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = emb\n",
    "X = sm.add_constant(X)\n",
    "y = train_df.reset_index().Rating\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "res = model.fit(cov_type=\"HC1\")\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "revised-violin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 2.121\n",
      "Mean squared error = 6.328\n",
      "Median absolute error = 2.079\n",
      "Explain variance score = 0.291\n",
      "R2 score = 0.291\n"
     ]
    }
   ],
   "source": [
    "pred = res.predict()\n",
    "\n",
    "print(\"Mean absolute error =\", round(skm.mean_absolute_error(y, pred), 3)) \n",
    "print(\"Mean squared error =\", round(skm.mean_squared_error(y, pred), 3)) \n",
    "print(\"Median absolute error =\", round(skm.median_absolute_error(y, pred), 3)) \n",
    "print(\"Explain variance score =\", round(skm.explained_variance_score(y, pred), 3)) \n",
    "print(\"R2 score =\", round(skm.r2_score(y, pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "challenging-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35433537428402134"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg = LogisticRegression(random_state=0).fit(X, y)\n",
    "LogReg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pleasant-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (train_df.Review.values, train_df.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "outdoor-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True)\n",
    "tk.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "greater-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tk.word_counts.keys())+1\n",
    "max_words = 300\n",
    "embedding_size = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "completed-custody",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My yardstick for measuring a movie's watch-abi...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many movies are there that you can think o...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I only went to see this movie because I have a...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was fortunate enough to see this movie on pr...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our family (and the entire sold out sneak prev...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>This movie was obviously made with a very low ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>CyberTracker is set in Los Angeles sometime in...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>Eric Phillips (Don Wilson) is a secret service...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>Plot Synopsis: Los Angeles in the future. Crim...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>Oh, dear! This has to be one of the worst film...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19988 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating  Sentiment\n",
       "0      My yardstick for measuring a movie's watch-abi...       7          1\n",
       "1      How many movies are there that you can think o...       7          1\n",
       "2      I only went to see this movie because I have a...       7          0\n",
       "3      I was fortunate enough to see this movie on pr...       7          1\n",
       "4      Our family (and the entire sold out sneak prev...       9          1\n",
       "...                                                  ...     ...        ...\n",
       "19983  This movie was obviously made with a very low ...       2          1\n",
       "19984  CyberTracker is set in Los Angeles sometime in...       3          1\n",
       "19985  Eric Phillips (Don Wilson) is a secret service...       3          0\n",
       "19986  Plot Synopsis: Los Angeles in the future. Crim...       4          0\n",
       "19987  Oh, dear! This has to be one of the worst film...       1          1\n",
       "\n",
       "[19988 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Check_set = test_df.Review.values\n",
    "Check_seq = tk.texts_to_sequences(Check_set)\n",
    "Check_pad = pad_sequences(Check_seq, maxlen = 100, padding = 'post')\n",
    "\n",
    "check_predict = model.predict_classes(Check_pad, verbose = 0)\n",
    "\n",
    "check_df = pd.DataFrame(list(zip(test_df.Review.values, test_df.Rating.values, check_predict)), columns = ['Review','Rating','Sentiment'])\n",
    "check_df.Sentiment = [1 if x == [1] else 0 for x in check_df.Sentiment]\n",
    "check_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-chosen",
   "metadata": {},
   "source": [
    "# 2. (evil) XOR Problem\n",
    "\n",
    "Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequence’s end. Test the two approaches below:\n",
    "\n",
    "### 2.1 \n",
    "\n",
    "Generate a dataset of random <=100,000 binary strings of equal length <= 50. Train the LSTM; what is the maximum length you can train up to with precisison?\n",
    "    \n",
    "\n",
    "### 2.2\n",
    "\n",
    "Generate a dataset of random <=200,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "authentic-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_key(p):\n",
    "    key = []\n",
    "    for i in range(p):\n",
    "        key.append(str(random.randint(0,1)))\n",
    "    return(key)\n",
    "\n",
    "def binary_dataset(n):\n",
    "    bin_list = []\n",
    "    for i in range(0, n):\n",
    "        bin_list.append(rand_key(50))\n",
    "    return(bin_list)\n",
    "\n",
    "def rand_binary_dataset(n):\n",
    "    rand_bin_list = []\n",
    "    rand = random.randint(1,50)\n",
    "    for i in range(0, n):\n",
    "        rand_bin_list.append(rand_key(rand))\n",
    "    return(rand_bin_list)\n",
    "\n",
    "def bin_to_num(s):\n",
    "    int1 = \"\"\n",
    "    for ele in s:\n",
    "        int1 += ele\n",
    "        return int(int1)\n",
    "    \n",
    "def getParity( n ):\n",
    "    parity = 0\n",
    "    while n:\n",
    "        parity = ~parity\n",
    "        n = n & (n - 1)\n",
    "    return abs(parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distant-holiday",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 3s 893us/step - loss: 0.0768 - binary_accuracy: 0.9024 2s - l\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 2s 771us/step - loss: 9.0424e-05 - binary_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 3s 853us/step - loss: 9.0016e-06 - binary_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 3s 826us/step - loss: 1.3406e-06 - binary_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 3s 840us/step - loss: 2.2821e-07 - binary_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 3s 829us/step - loss: 4.1315e-08 - binary_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 3s 813us/step - loss: 8.5714e-09 - binary_accuracy: 1.0000 1s - loss: 1.\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 3s 802us/step - loss: 2.3931e-09 - binary_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 3s 831us/step - loss: 9.2783e-10 - binary_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 3s 814us/step - loss: 4.8550e-10 - binary_accuracy: 1.0000 0s - loss: 5.0866e-10 - bina\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_list = binary_dataset(100000)\n",
    "\n",
    "num = []\n",
    "for i in range(0, len(bin_list)):\n",
    "    par = []\n",
    "    par.append(getParity(bin_to_num(bin_list[i])))\n",
    "    num.append(par)\n",
    "\n",
    "training_data = np.array(bin_list, \"float32\")\n",
    "\n",
    "target_data = np.array(num, \"float32\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(training_data, target_data, epochs=10, verbose=1)\n",
    "\n",
    "model.predict(training_data).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "everyday-museum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 50 but received input with shape (32, 10)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-77ad377f8b0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m               metrics=['binary_accuracy'])\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\gaia_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 50 but received input with shape (32, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_bin_list = rand_binary_dataset(100000)\n",
    "\n",
    "rand_num = []\n",
    "for i in range(0, len(rand_bin_list)):\n",
    "    rand_par = []\n",
    "    rand_par.append(getParity(bin_to_num(rand_bin_list[i])))\n",
    "    rand_num.append(rand_par)\n",
    "\n",
    "training_data = np.array(rand_bin_list, \"float32\")\n",
    "\n",
    "target_data = np.array(rand_num, \"float32\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(training_data, target_data, epochs=10, verbose=1)\n",
    "\n",
    "model.predict(training_data).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-venezuela",
   "metadata": {},
   "source": [
    "<font color=green>The randomized lengths of binary numbers does not run properly and seems to cause issue in Keras. Model can't seem to run properly if the entries are not consistent. The first part of the question, every single entry was consistently length 50 whereas the second part has lengths ranging from 1 to 50. Ran the input code and left the error message on purpose to show result.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
